{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size*top_n, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size*top_n, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size*top_n, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_base(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Encoder_base, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder_base(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        x = self.dropout(x, training=True)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, True, False)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未使用\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, num_labels,\n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.out = tf.keras.layers.Dense(self.num_labels)\n",
    "\n",
    "    def call(self, x): #, training, mask):\n",
    "        \n",
    "        batch = tf.shape(x)[0]\n",
    "        topn = tf.shape(x)[1]\n",
    "        seq_len = tf.shape(x)[2]\n",
    "        \n",
    "        x = tf.reshape(x, (batch, topn, -1))\n",
    "        \n",
    "        # adding embedding and position encoding.\n",
    "        # x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        # x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # x = self.dropout(x, training=training)\n",
    "        x = self.dropout(x, training=True)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # x = self.enc_layers[i](x, training, mask)\n",
    "            x = self.enc_layers[i](x, True, False)\n",
    "        \n",
    "        x = tf.reshape(x, (batch, topn, seq_len, -1))\n",
    "        \n",
    "        # 添加 self-attention\n",
    "        \n",
    "        # 添加softmax\n",
    "        x = tf.reshape(x, (batch, -1))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x  # (batch_size, labels) // (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=500, num_heads=5, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 20, 10*50)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "**Macro-f1**:  \n",
    "$\\text{Macro}-F_{1}=\\frac{1}{|\\mathcal{S}|} \\sum_{t \\in \\mathcal{S}} \\frac{2 P_{t} R_{t}}{P_{t}+R_{t}} \\\\\n",
    "P_{t}=\\frac{T P_{t}}{T P_{t}+F P_{t}} \\\\\n",
    "R_{t}=\\frac{T P_{t}}{T P_{t}+F N_{t}}$  \n",
    "\n",
    "\n",
    "**Micro-f1**:  \n",
    "$Micro-F_{1}=\\frac{2 P R}{P+R} \\\\ \n",
    "\\text{Precision(P)}=\\frac{\\sum_{t \\in \\mathcal{S}} T P_{t}}{\\sum_{t \\in \\mathcal{S}} T P_{t}+F P_{t}} \\\\\n",
    "\\text{Recall}(R)=\\frac{\\sum_{t \\in \\mathcal{S}} T P_{t}}{\\sum_{t \\in \\mathcal{S}} T P_{t}+F N_{t}}\n",
    "$\n",
    "\n",
    "``` ptyhon\n",
    "y_true = np.array([[1,0,1,1,0],[1,1,0,1,1]])\n",
    "y_pred = np.array([[0,1,1,1,0],[1,1,1,0,1]])\n",
    "\n",
    "macro_f1 0.7333333333333333\n",
    "micro_f1 0.7142857142857143\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "class Metrics(Callback):\n",
    "    \n",
    "    # model.fit can't support auto set valdidation_data\n",
    "    # handle set val_data in __init__\n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        \n",
    "    def f1(self, y_true, y_pred):\n",
    "        assert np.shape(y_true) == np.shape(y_pred)\n",
    "        \n",
    "        tp = np.sum(y_true * y_pred, axis = 0)\n",
    "        tpfp = np.sum(y_true, axis=0)\n",
    "        tpfn = np.sum(y_pred, axis=0)\n",
    "\n",
    "        \"\"\"Macro_F1 metric.\n",
    "        \n",
    "        for each label, compute a binary classification f1, and then,\n",
    "        average all of them.\n",
    "        \"\"\"\n",
    "        p = tp / tpfp \n",
    "        r = tp / tpfn\n",
    "        macro_f1 = np.mean(2*p*r/(p+r))\n",
    "\n",
    "        \"\"\"Micro_F1 metric.\n",
    "        \n",
    "        Statitic all TPs, FPs, FNs, and then use f1 fomulation.\n",
    "        \n",
    "        \"\"\"\n",
    "        p = np.sum(tp)/np.sum(tpfp)\n",
    "        r = np.sum(tp)/np.sum(tpfn)\n",
    "        micro_f1 = 2*p*r/(p+r)\n",
    "\n",
    "        return macro_f1, micro_f1\n",
    "\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.val_micro_f1s = []\n",
    "        self.val_macro_f1s = []\n",
    "        #self.val_recalls = []\n",
    "        #self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(\n",
    "            self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        _val_macro_f1,_val_micro_f1 = self.f1(val_targ, val_predict)\n",
    "        self.val_macro_f1s.append(_val_macro_f1)\n",
    "        self.val_micro_f1s.append(_val_micro_f1)\n",
    "        #self.val_recalls.append(_val_recall)\n",
    "        #self.val_precisions.append(_val_precision)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = seq_lens*embedding\n",
    "topn = 100\n",
    "seq_lens = 20\n",
    "embedding = 50\n",
    "\n",
    "num_layers = 1\n",
    "d_model = seq_lens * embedding\n",
    "dff = 512\n",
    "num_heads = 10\n",
    "num_labels = 126\n",
    "dff = 1024\n",
    "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 11s 3s/sample - loss: 1.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nocater/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/nocater/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/nocater/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/nocater/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "4/4 [==============================] - 8s 2s/sample - loss: 0.7625\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "encoder_3 (Encoder)          multiple                  6058024   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             multiple                  12600126  \n",
      "=================================================================\n",
      "Total params: 18,658,150\n",
      "Trainable params: 18,658,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "tff = tf.keras.Sequential()\n",
    "tff.add(tf.keras.layers.Reshape((topn,d_model)))\n",
    "tff.add(Encoder(num_layers, d_model, num_heads, dff, dropout_rate))\n",
    "tff.add(tf.keras.layers.Flatten())\n",
    "tff.add(tf.keras.layers.Dense(num_labels))\n",
    "\n",
    "tff.compile(loss=tf.losses.sigmoid_cross_entropy,\n",
    "              optimizer='adam')\n",
    "\n",
    "# x = np.random.uniform(size=(64,topn,seq_lens,embedding)) # batch,topn,sequence_length,embdding_dim\n",
    "# y = np.random.randint(num_labels,size=(64))\n",
    "\n",
    "# dont use auto split train valid dataset in .fit\n",
    "val_metric = Metrics([x[-2:],y[-2:]])\n",
    "his = tff.fit(x[:4],y[:4],\n",
    "          batch_size=2,\n",
    "          epochs=2,\n",
    "          callbacks=[val_metric])\n",
    "\n",
    "tff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1551, 100, 20, 50), (1551, 126))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "PATH = r'/mnt/d/Dataset/rcv1/'\n",
    "\n",
    "# for file in os.listdir(PATH+'matrix/'):\n",
    "x = np.load(PATH+'/matrix/range2000_4000.npy')\n",
    "y = np.load(PATH+'/label/label_range2000_4000.npy')\n",
    "x.shape,y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
