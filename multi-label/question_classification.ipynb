{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高中政治"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'时事政治Done!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['经济学常识', '科学思维常识', '生活中的法律常识','科学社会主义常识','公民道德与伦理常识','时事政治']\n",
    "category = '时事政治'\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_政治/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_政治/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高中历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'现代史Done!2139'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['古代史', '近代史', '现代史']\n",
    "category = labels[2]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_历史/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_历史/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高中-地理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'地球与地图Done!78'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['地球与地图','宇宙中的地球','生产活动与地域联系','人口与城市','区域可持续发展']\n",
    "category = labels[0]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_地理/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_地理/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高中-生物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'现代生物技术专题Done!688'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['现代生物技术专题','生物科学与社会','生物技术实践','稳态与环境','遗传与进化','分子与细胞']\n",
    "category = labels[0]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_生物/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_生物/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17920,\n",
       " 17920,\n",
       " {'高中_地理': 0,\n",
       "  '高中_历史': 1,\n",
       "  '高中_生物': 2,\n",
       "  '高中_政治': 3,\n",
       "  '高中_政治_公民道德与伦理常识': 4,\n",
       "  '高中_历史_近代史': 5,\n",
       "  '高中_历史_现代史': 6,\n",
       "  '高中_地理_人口与城市': 7,\n",
       "  '高中_地理_宇宙中的地球': 8,\n",
       "  '高中_生物_分子与细胞': 9,\n",
       "  '高中_生物_稳态与环境': 10,\n",
       "  '高中_生物_生物技术实践': 11,\n",
       "  '高中_生物_生物科学与社会': 12},\n",
       " {'高中_政治_公民道德与伦理常识': 1760,\n",
       "  '高中_历史_近代史': 1456,\n",
       "  '高中_历史_现代史': 2139,\n",
       "  '高中_地理_人口与城市': 1068,\n",
       "  '高中_地理_宇宙中的地球': 2742,\n",
       "  '高中_生物_分子与细胞': 2241,\n",
       "  '高中_生物_稳态与环境': 2520,\n",
       "  '高中_生物_生物技术实践': 1257,\n",
       "  '高中_生物_生物科学与社会': 2737},\n",
       " 17920)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = r'/mnt/d/Dataset/百度题库/'\n",
    "label_id = dict(zip(['高中_地理','高中_历史','高中_生物','高中_政治'],range(4)))\n",
    "label_count = {}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for label in os.listdir(path):\n",
    "    for doc in os.listdir(path+label):\n",
    "        if not doc.endswith('csv'):\n",
    "            continue\n",
    "        \n",
    "        sublabel = os.path.splitext(doc)[0]\n",
    "        data = open(path+label+'/'+doc).readlines()\n",
    "        data = list(map(lambda x: x[4:], data))\n",
    "        \n",
    "        if len(data)<1000:continue\n",
    "        \n",
    "        x.extend(data)\n",
    "        label_count[label+'_'+sublabel] = len(data)\n",
    "        label_id[label+'_'+sublabel] = len(label_id)\n",
    "        y.extend([[label_id[label], label_id[label+'_'+sublabel]]]*len(data))\n",
    "\n",
    "# 去除内容中的制表符\n",
    "x = [''.join(i.split()) for i in x]\n",
    "\n",
    "\n",
    "len(x), len(y), label_id, label_count, sum(label_count.values())\n",
    "# with open('./x.txt', 'w') as f:\n",
    "#     f.write(''.join(x))\n",
    "# with open('./y.txt', 'w') as f:\n",
    "#     f.write(repr(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_id.json','w') as f:\n",
    "    f.write(repr(label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "x_ = []\n",
    "no = 1\n",
    "for i in x:\n",
    "    words = pseg.cut(i)\n",
    "    data = [word for word,flag in words if flag != 'x' ]\n",
    "    with open(path+'segs/'+str(no)+'.txt','w') as f:\n",
    "        f.write(' '.join(data))\n",
    "    \n",
    "    no+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to_tsv single label for baidu.ernie\n",
    "先试试8分类任务  \n",
    "已跑通 效果acc:0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "indexs = list(range(len(x)))\n",
    "np.random.shuffle(indexs)\n",
    "x_,y_ = np.array(x)[indexs], np.array(y)[indexs]\n",
    "\n",
    "\n",
    "train_x, train_y = x_[:-4000],y_[:-4000]\n",
    "dev_x, dev_y = x_[-4000:-2000],y_[-4000:-2000]\n",
    "test_x, test_y = x_[-2000:],y_[-2000:]\n",
    "\n",
    "with open('./data/kkb/test.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(test_x, test_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "        \n",
    "with open('./data/kkb/dev.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(dev_x, dev_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "\n",
    "with open('./data/kkb/train.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    i = 0\n",
    "    for (a,b) in zip(train_x, train_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to_tsv multi labels\n",
    "13分类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "indexs = list(range(len(x)))\n",
    "np.random.shuffle(indexs)\n",
    "x_,y_ = np.array(x)[indexs], np.array(y)[indexs]\n",
    "\n",
    "\n",
    "train_x, train_y = x_[:-4000],y_[:-4000]\n",
    "dev_x, dev_y = x_[-4000:-2000],y_[-4000:-2000]\n",
    "test_x, test_y = x_[-2000:],y_[-2000:]\n",
    "\n",
    "with open('./data/kkb/test_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(test_x, test_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "        \n",
    "with open('./data/kkb/dev_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(dev_x, dev_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "\n",
    "with open('./data/kkb/train_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    i = 0\n",
    "    for (a,b) in zip(train_x, train_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert_keras多标签分类示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import re, os\n",
    "import codecs\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "config_path = 'model/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'model/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'model/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "\n",
    "\n",
    "neg = pd.read_excel('./data/bert_keras/neg.xls', header=None)\n",
    "pos = pd.read_excel('./data/bert_keras/pos.xls', header=None)\n",
    "\n",
    "data = []\n",
    "\n",
    "for d in neg[0]:\n",
    "    data.append((d, 0))\n",
    "\n",
    "for d in pos[0]:\n",
    "    data.append((d, 1))\n",
    "\n",
    "\n",
    "# 按照9:1的比例划分训练集和验证集\n",
    "random_order = list(range(len(data)))\n",
    "np.random.shuffle(random_order)\n",
    "train_data = [data[j] for i, j in enumerate(random_order) if i % 10 != 0]\n",
    "valid_data = [data[j] for i, j in enumerate(random_order) if i % 10 == 0]\n",
    "\n",
    "\n",
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=32):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, Y = [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0][:maxlen]\n",
    "                x1, x2 = tokenizer.encode(first=text)\n",
    "                y = d[1]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                Y.append([y])\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    Y = seq_padding(Y)\n",
    "                    yield [X1, X2], Y\n",
    "                    [X1, X2, Y] = [], [], []\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "x1_in = Input(shape=(None,))\n",
    "x2_in = Input(shape=(None,))\n",
    "\n",
    "x = bert_model([x1_in, x2_in])\n",
    "x = Lambda(lambda x: x[:, 0])(x)\n",
    "p = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([x1_in, x2_in], p)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-5), # 用足够小的学习率\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_D = data_generator(train_data)\n",
    "valid_D = data_generator(valid_data)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_D.__iter__(),\n",
    "    steps_per_epoch=len(train_D),\n",
    "    epochs=5,\n",
    "    validation_data=valid_D.__iter__(),\n",
    "    validation_steps=len(valid_D)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
