{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带知识点label处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高中 地理 地球与地图 \tsize:431\n",
      "高中 地理 宇宙中的地球 \tsize:3716\n",
      "高中 地理 生产活动与地域联系 \tsize:1340\n",
      "高中 地理 人口与城市 \tsize:1570\n",
      "高中 地理 区域可持续发展 \tsize:130\n",
      "高中 历史 古代史 \tsize:1000\n",
      "高中 历史 近代史 \tsize:1640\n",
      "高中 历史 现代史 \tsize:2330\n",
      "高中 生物 现代生物技术专题 \tsize:1000\n",
      "高中 生物 生物科学与社会 \tsize:3900\n",
      "高中 生物 生物技术实践 \tsize:1770\n",
      "高中 生物 稳态与环境 \tsize:3570\n",
      "高中 生物 遗传与进化 \tsize:1040\n",
      "高中 生物 分子与细胞 \tsize:2980\n",
      "高中 政治 经济学常识 \tsize:566\n",
      "高中 政治 科学思维常识 \tsize:260\n",
      "高中 政治 生活中的法律常识 \tsize:170\n",
      "高中 政治 科学社会主义常识 \tsize:573\n",
      "高中 政治 公民道德与伦理常识 \tsize:1760\n",
      "高中 政治 时事政治 \tsize:67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29813,                                            labels  \\\n",
       " 163              [高中, 地理, 地球与地图, 等值线图, 地球运动的基本形式]   \n",
       " 335                [高中, 地理, 地球与地图, 等值线图, 常见的天气系统]   \n",
       " 378       [高中, 地理, 地球与地图, 大洋洲和南极洲, 地球仪，经纬网及其地理意义]   \n",
       " 118  [高中, 地理, 地球与地图, 等高线地形图、地形剖面图, 地球仪，经纬网及其地理意义]   \n",
       " 27   [高中, 地理, 地球与地图, 等高线地形图、地形剖面图, 地球仪，经纬网及其地理意义]   \n",
       " \n",
       "                                                   item  \n",
       " 163  下图是“地球自转等线速度示意图”，R、T在同一纬线上。据此回答下面小题。1.该区域所在的半球...  \n",
       " 335  下图为“我国东南沿海某地区示意图”，关于此时A地的叙述，正确的是（）①雨水汇集，泛滥成灾②受...  \n",
       " 378  中国第26次南极科考队经中山站(69°22′S,76°22′E)到昆仑站(80°25′S,7...  \n",
       " 118  读下面经纬网图和等高线图，判断下列说法正确的是()A甲图全部在西半球，乙图在北半球B甲图比例...  \n",
       " 27   茶树喜湿怕涝，海拔1500ｍ左右排水良好的山地缓坡，往往成为优质高山茶的重要产区，下图示意我...  )"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "grades = ['高中']\n",
    "subjects = ['地理', '历史', '生物', '政治']\n",
    "categories = {  '地理':['地球与地图','宇宙中的地球','生产活动与地域联系','人口与城市','区域可持续发展'],\n",
    "                '历史':['古代史', '近代史', '现代史'],\n",
    "                '生物':['现代生物技术专题','生物科学与社会','生物技术实践','稳态与环境','遗传与进化','分子与细胞'],\n",
    "                '政治':['经济学常识', '科学思维常识', '生活中的法律常识','科学社会主义常识','公民道德与伦理常识','时事政治']\n",
    "              }\n",
    "\n",
    "df_target = pd.DataFrame(columns=['labels','item'])\n",
    "for grade in grades:\n",
    "    for subject in subjects:\n",
    "        for category in categories[subject]:\n",
    "            file = r'/mnt/d/Dataset/百度题库/'+grade+'_'+subject+'/origin/'+category+'.csv'\n",
    "            df = pd.read_csv(file)\n",
    "            print(f'{grade} {subject} {category} \\tsize:{len(df)}')\n",
    "\n",
    "            # 按网页顺序对其排序\n",
    "            df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "            df = df[['web-scraper-order','item']]\n",
    "            df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "            # 对文本处理\n",
    "            def foo(x):\n",
    "                x = \"\".join(x.split())\n",
    "                # x = x[:x.index('题型')]\n",
    "                return x\n",
    "\n",
    "            # 删除文本中的换行\n",
    "            df['item'] = df.item.apply(lambda x:  \"\".join(x.split()))\n",
    "            df['labels'] = df.item.apply(lambda x: [grade, subject, category]+x[x.index('[知识点：]')+6:].split(',') if x.find('[知识点：]')!=-1 else [grade, subject, category])\n",
    "            df['item'] = df.item.apply(lambda x:  x.replace('[题目]',''))\n",
    "            df['item'] = df.item.apply(lambda x:  x[:x.index('题型')])\n",
    "\n",
    "            df = df[['labels','item']]\n",
    "            df_target = df_target.append(df)\n",
    "\n",
    "len(df_target),df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">300 datasize:22576 multi_class:95\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 设置样本数量阈值\n",
    "min_samples = 300\n",
    "# 阈值 标签数\n",
    "# 500   64\n",
    "# 400   75\n",
    "# 300   94\n",
    "# 200   134\n",
    "# 100   228\n",
    "\n",
    "df = df_target.copy()\n",
    "labels = []\n",
    "for i in df.labels:\n",
    "    labels.extend(i)\n",
    "    \n",
    "result = dict(sorted(dict(Counter(labels)).items(), key=lambda x:x[1], reverse=True))\n",
    "lens = np.array(list(result.values()))\n",
    "LABEL_NUM = len(lens[lens>min_samples])\n",
    "\n",
    "# 选定数据label\n",
    "label_target = set([k for k,v in result.items() if v > min_samples])\n",
    "\n",
    "# \n",
    "df['labels'] = df.labels.apply(lambda x: x[:3] + list(set(x) - set(x[:3]) & label_target)) # 保证 grade subject category 在前三位置\n",
    "df['labels'] = df.labels.apply(lambda x: None if len(x)<4 else x)                          # 去除没有知识点的数据\n",
    "df = df[df.labels.notna()]\n",
    "\n",
    "# 最终的labels数量\n",
    "labels = []\n",
    "[labels.extend(i) for i in df.labels]\n",
    "LABEL_NUM = len(set(labels))\n",
    "\n",
    "print(f'>{min_samples} datasize:{len(df)} multi_class:{LABEL_NUM}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理成fasttext格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext data file generated!  /mnt/d/Dataset/百度题库/baidu_95__label__.txt\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "profix = '__label__'\n",
    "    \n",
    "if profix:\n",
    "    df['labels'] = df.labels.apply(lambda x: [profix+i for i in x])\n",
    "\n",
    "df['labels'] = df.labels.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# shuffle\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "file = rf'/mnt/d/Dataset/百度题库/baidu_{LABEL_NUM}{profix}.txt'\n",
    "\n",
    "with open(file, 'w') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(row['labels'] + ' ' +row['item'] +'\\n')\n",
    "print('fasttext data file generated! ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理成csv格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv data file generated!  /mnt/d/Dataset/百度题库/baidu_95.csv\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "profix = ''\n",
    "    \n",
    "if profix:\n",
    "    df['labels'] = df.labels.apply(lambda x: [profix+i for i in x])\n",
    "\n",
    "df['labels'] = df.labels.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# shuffle\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "file = rf'/mnt/d/Dataset/百度题库/baidu_{LABEL_NUM}{profix}.csv'\n",
    "\n",
    "df.to_csv(file, index=False, sep=',', header=False, encoding='UTF8') # 当sep 字符在df中存在会在字符串前后添加引号\n",
    "print('csv data file generated! ', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find words: 50726\n",
      "Vocabulary size: 50727\n",
      "Shape of train data: (22576, 200)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "import jieba\n",
    "padding_size = 200\n",
    "\n",
    "df = pd.read_csv('/mnt/d/Dataset/百度题库/baidu_95.csv', header=None, names=[\"labels\", \"item\"], dtype=str)\n",
    "df['item'] = df.item.apply(lambda x: list(jieba.cut(x)))\n",
    "corpus = df.item.tolist()\n",
    "text_preprocesser = preprocessing.text.Tokenizer(num_words=5000)\n",
    "text_preprocesser.fit_on_texts(corpus)\n",
    "x = text_preprocesser.texts_to_sequences(corpus)\n",
    "word_dict = text_preprocesser.word_index\n",
    "# json.dump(word_dict, open(vocab_file, 'w'), ensure_ascii=False)\n",
    "# max_doc_length = max([len(each_text) for each_text in x])\n",
    "x = preprocessing.sequence.pad_sequences(x, maxlen=padding_size,\n",
    "                                         padding='post', truncating='post')\n",
    "\n",
    "# for y\n",
    "print(\"Find words: {:d}\".format(len(word_dict)))\n",
    "print(\"Vocabulary size: {:d}\".format(vocab_size))\n",
    "print(\"Shape of train data: {}\".format(np.shape(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': None,\n",
       " 'sparse_output': False,\n",
       " 'classes_': array(['“重农抑商”政策', '不完全显性', '与细胞分裂有关的细胞器', '中央官制——三公九卿制', '中心体的结构和功能',\n",
       "        '人体免疫系统在维持稳态中的作用', '人体水盐平衡调节', '人体的体温调节', '人口与城市', '人口增长与人口问题',\n",
       "        '人口迁移与人口流动', '人工授精、试管婴儿等生殖技术', '伴性遗传', '体液免疫的概念和过程', '免疫系统的功能',\n",
       "        '免疫系统的组成', '公民道德与伦理常识', '兴奋在神经元之间的传递', '兴奋在神经纤维上的传导', '内环境的稳态',\n",
       "        '内质网的结构和功能', '农业区位因素', '减数分裂与有丝分裂的比较', '减数分裂的概念', '分子与细胞',\n",
       "        '劳动就业与守法经营', '历史', '古代史', '器官移植', '地球与地图', '地球所处的宇宙环境',\n",
       "        '地球的内部圈层结构及特点', '地球的外部圈层结构及特点', '地球运动的地理意义', '地球运动的基本形式', '地理',\n",
       "        '垄断组织的出现', '培养基与无菌技术', '基因工程的原理及技术', '基因工程的概念', '基因的分离规律的实质及应用',\n",
       "        '基因的自由组合规律的实质及应用', '复等位基因', '夏商两代的政治制度', '太阳对地球的影响', '宇宙中的地球',\n",
       "        '工业区位因素', '拉马克的进化学说', '政治', '文艺的春天', '核糖体的结构和功能', '海峡两岸关系的发展',\n",
       "        '液泡的结构和功能', '清末民主革命风潮', '溶酶体的结构和功能', '激素调节', '现代史', '现代生物技术专题',\n",
       "        '生产活动与地域联系', '生命活动离不开细胞', '生态系统的营养结构', '生活中的法律常识', '生物', '生物工程技术',\n",
       "        '生物性污染', '生物技术在其他方面的应用', '生物技术实践', '生物科学与社会', '皇帝制度',\n",
       "        '社会主义市场经济的伦理要求', '社会主义是中国人民的历史性选择', '神经调节和体液调节的比较', '科学社会主义常识',\n",
       "        '稳态与环境', '第三产业的兴起和“新经济”的出现', '组成细胞的化合物', '组成细胞的化学元素',\n",
       "        '细胞大小与物质运输的关系', '细胞有丝分裂不同时期的特点', '细胞的多样性和统一性', '经济学常识', '群落的结构',\n",
       "        '胚胎移植', '蛋白质的合成', '血糖平衡的调节', '走进细胞', '近代史', '选官、用官制度的变化', '遗传与进化',\n",
       "        '遗传的分子基础', '遗传的细胞基础', '避孕的原理和方法', '郡县制', '高中', '高尔基体的结构和功能'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "y = df.labels.apply(lambda x: set(x.split())).tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y)\n",
    "mlb.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 不带知识点数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高中政治"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'时事政治Done!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['经济学常识', '科学思维常识', '生活中的法律常识','科学社会主义常识','公民道德与伦理常识','时事政治']\n",
    "category = '时事政治'\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_政治/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_政治/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高中历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 2330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'现代史Done!2139'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['古代史', '近代史', '现代史']\n",
    "category = labels[2]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_历史/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_历史/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高中-地理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'地球与地图Done!78'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['地球与地图','宇宙中的地球','生产活动与地域联系','人口与城市','区域可持续发展']\n",
    "category = labels[0]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_地理/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_地理/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高中-生物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'现代生物技术专题Done!688'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['现代生物技术专题','生物科学与社会','生物技术实践','稳态与环境','遗传与进化','分子与细胞']\n",
    "category = labels[0]\n",
    "\n",
    "file = r'/mnt/d/Dataset/百度题库/高中_生物/origin/'+category+'.csv'\n",
    "df = pd.read_csv(file)\n",
    "print('size:', len(df))\n",
    "\n",
    "# 按网页顺序对其排序\n",
    "df['web-scraper-order'] = df['web-scraper-order'].apply(lambda x: int(x.split('-')[1]))\n",
    "df = df[['web-scraper-order','item']]\n",
    "df = df.sort_values(by='web-scraper-order')\n",
    "\n",
    "# 对文本处理\n",
    "def foo(x):\n",
    "    x = x.strip()\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('\\t','')\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x.replace(' ','')\n",
    "    x = x[:x.index('题型')]\n",
    "    return x\n",
    "\n",
    "# 删除文本中的换行\n",
    "df['item'] = df.item.apply(lambda x: foo(x))\n",
    "# 删除带图数据\n",
    "df = df[df.item.str.contains('图')==False]\n",
    "df = df['item']\n",
    "\n",
    "# save\n",
    "with open(r'/mnt/d/Dataset/百度题库/高中_生物/'+category+'.csv','w',encoding='utf8') as f:\n",
    "    f.write('\\n'.join(list(df.values)))\n",
    "category+'Done!'+str(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = r'/mnt/d/Dataset/百度题库/'\n",
    "label_id = dict(zip(['高中_地理','高中_历史','高中_生物','高中_政治'],range(4)))\n",
    "label_count = {}\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for label in os.listdir(path):\n",
    "    for doc in os.listdir(path+label):\n",
    "        if not doc.endswith('csv'):\n",
    "            continue\n",
    "        \n",
    "        sublabel = os.path.splitext(doc)[0]\n",
    "        data = open(path+label+'/'+doc).readlines()\n",
    "        data = list(map(lambda x: x[4:], data))\n",
    "        \n",
    "        if len(data)<500:continue\n",
    "        \n",
    "        x.extend(data)\n",
    "        label_count[label+'_'+sublabel] = len(data)\n",
    "        label_id[label+'_'+sublabel] = len(label_id)\n",
    "        y.extend([[label_id[label], label_id[label+'_'+sublabel]]]*len(data))\n",
    "\n",
    "# 去除内容中的制表符\n",
    "x = [''.join(i.split()) for i in x]\n",
    "\n",
    "\n",
    "len(x), len(y), label_id, label_count, sum(label_count.values())\n",
    "with open('./data/kkb/x_19.txt', 'w') as f:\n",
    "    f.write('\\n'.join(x))\n",
    "with open('./data/kkb/y_19.txt', 'w') as f:\n",
    "    f.write(repr(y))\n",
    "with open('label_id_19.json','w') as f:\n",
    "    f.write(repr(label_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of words\n",
    "not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def count_words(s):\n",
    "    stop_words = ['$', '?', '_', '“', '”', '、', '。', '《', '》','，','（', '）', '的', '了', '是']\n",
    "    tokenstr = []\n",
    "    result = {}\n",
    "    \n",
    "    word2pos = {}\n",
    "    pos2word = {}\n",
    "    \n",
    "    words = jieba.cut(s)\n",
    "    \n",
    "    i = 0 \n",
    "    for word in words:\n",
    "        if word in stop_words: continue\n",
    "        tokenstr.append(word)\n",
    "        result[word] = result.get(word, 0) + 1\n",
    "        pos2word[i] = word\n",
    "        \n",
    "        indexs = word2pos.get(word, [])\n",
    "        indexs.append(i)\n",
    "        word2pos[word] = indexs\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    result = dict(sorted(result.items(), key=lambda x: (x[1],x[0]), reverse=True))\n",
    "    wordslist = list(result.keys())\n",
    "    assert len(set(tokenstr)) == len(wordslist)\n",
    "    return (wordslist, tokenstr, word2pos, pos2word)\n",
    "\n",
    "\n",
    "def fill_table(TD_list, related_tables,target_width, qqueue):\n",
    "    TD_list[0] = qqueue[0] # TD_list 长度为target_width 第一个位置对应此单词在wlist中的索引。0,1,2...\n",
    "    count = 1\n",
    "\n",
    "    while qqueue != [] and count < target_width:\n",
    "        use_index = qqueue[0] # 单词索引\n",
    "        del qqueue[0]\n",
    "        use_list = related_tables[use_index]  #取出use_index单词对应的相关单词。\n",
    "        len1 = len(use_list)   # 查看 i对应 的相关单词的个数。\n",
    "        len2 = target_width - count \n",
    "        if len1 >= len2:   # 大体意思应该是查看单词i对应的相关单词个数如果满足 target_width就直接从相关单词按顺序取出来填充到TD_list中。\n",
    "            TD_list[count:] = use_list[:len2]  \n",
    "            assert len(TD_list) == target_width\n",
    "            count = target_width\n",
    "            break\n",
    "        else:              # 如果不满足就有多少填多少。剩下的用 -1填充。\n",
    "            TD_list[count:count + len1] = use_list\n",
    "            assert len(TD_list) == target_width\n",
    "            count += len1\n",
    "            for next_id in use_list:\n",
    "                qqueue.append(next_id)\n",
    "    for i in range(count, target_width):\n",
    "        TD_list[i] = -1\n",
    "        \n",
    "        \n",
    "def reorder(table, word2pos, pos2word, wlist, word2id):\n",
    "    sort_table = []\n",
    "    topn, neighbor = np.array(table).shape\n",
    "    for i in range(topn):\n",
    "        tmp = []\n",
    "        tmp += word2pos[wlist[table[i][0]]] # record each center word index\n",
    "        length = len(tmp)                   # occurred times of center words\n",
    "        t = []                              # t is use to related words index\n",
    "        for j in range(1, neighbor):\n",
    "            t += word2pos[wlist[table[i][j]]]\n",
    "        index = np.random.randint(len(t), size = 20-length)\n",
    "        t = np.array(t)\n",
    "        t = list(t[index])\n",
    "        tmp = tmp + t                       # conccat the index of center word and index of its related words\n",
    "        tmp.sort()\n",
    "        for j in range(len(tmp)):\n",
    "            tmp[j] = word2id[pos2word[tmp[j]]] # convert index to word_id\n",
    "            # tmp[j] = pos2word[tmp[j]]       # convert index to word\n",
    "        sort_table.append(tmp)\n",
    "    \n",
    "    return np.array(sort_table)\n",
    "\n",
    "\n",
    "def text2matrix(s, sliding_window=3, target_width=5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    (wlist, tokenwords, word2pos, pos2word) = count_words(s)\n",
    "    word2id = {k:v for k,v in zip(wlist, range(len(wlist)))}\n",
    "    wordslist_length = len(wlist)\n",
    "    \n",
    "    AM_table = [[0 for i in range(wordslist_length)] for j in range(wordslist_length)]\n",
    "    \n",
    "    # generate occurred matrix with sliding_window\n",
    "    for num in range(len(tokenwords)-sliding_window+1):\n",
    "        for i in range(sliding_window-1):\n",
    "            for j in range(i+1, sliding_window):\n",
    "                AM_table[wlist.index(tokenwords[num + i])][wlist.index(tokenwords[num + j])] += 1\n",
    "                AM_table[wlist.index(tokenwords[num + j])][wlist.index(tokenwords[num + i])] += 1\n",
    "                \n",
    "    related_tables = {}\n",
    "    for i in range(wordslist_length):\n",
    "        related_tables[i] = [[index, num] for index, num in enumerate(AM_table[i]) if num > 0 and index != i]\n",
    "        related_tables[i].sort(key=lambda x: x[1], reverse=True)\n",
    "        related_tables[i] = [element[0] for element in related_tables[i]]\n",
    "    \n",
    "    TD_table = [[-1 for i in range(target_width)] for j in range(wordslist_length)]\n",
    "    for i in range(wordslist_length):\n",
    "        fill_table(TD_table[i], related_tables, target_width, [i]) # fill TD table with -1\n",
    "    \n",
    "    # TD_table = reorder(TD_table, word2pos, pos2word, wlist, word2id)\n",
    "    \n",
    "    # convert id to words: arrange word matrix\n",
    "    awm = []\n",
    "    for row in TD_table:\n",
    "        awm.append([pos2word[i] for i in row])\n",
    "    return wlist, awm # ,TD_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# way1(ERNIE): to_tsv single label for baidu.ernie\n",
    "先试试8分类任务  \n",
    "已跑通 效果acc:0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "indexs = list(range(len(x)))\n",
    "np.random.shuffle(indexs)\n",
    "x_,y_ = np.array(x)[indexs], np.array(y)[indexs]\n",
    "\n",
    "\n",
    "train_x, train_y = x_[:-4000],y_[:-4000]\n",
    "dev_x, dev_y = x_[-4000:-2000],y_[-4000:-2000]\n",
    "test_x, test_y = x_[-2000:],y_[-2000:]\n",
    "\n",
    "with open('./data/kkb/test.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(test_x, test_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "        \n",
    "with open('./data/kkb/dev.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(dev_x, dev_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "\n",
    "with open('./data/kkb/train.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    i = 0\n",
    "    for (a,b) in zip(train_x, train_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(b[1]-4)+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# way1(ERNIE):to_tsv multi labels\n",
    "13分类数据  \n",
    "so badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "indexs = list(range(len(x)))\n",
    "np.random.shuffle(indexs)\n",
    "x_,y_ = np.array(x)[indexs], np.array(y)[indexs]\n",
    "\n",
    "\n",
    "train_x, train_y = x_[:-4000],y_[:-4000]\n",
    "dev_x, dev_y = x_[-4000:-2000],y_[-4000:-2000]\n",
    "test_x, test_y = x_[-2000:],y_[-2000:]\n",
    "\n",
    "with open('./data/kkb/test_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(test_x, test_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "        \n",
    "with open('./data/kkb/dev_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    for (a,b) in zip(dev_x, dev_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)\n",
    "\n",
    "with open('./data/kkb/train_multi.tsv','w',encoding='utf8') as f:\n",
    "    f.write('label\\ttext_a\\n')\n",
    "    i = 0\n",
    "    for (a,b) in zip(train_x, train_y):\n",
    "        a = ''.join(a.split())+'\\n'\n",
    "        f.write(str(list(b))+'\\t'+a)\n",
    "        # f.write(repr(list(b))+'\\t'+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# way2(BERT):\n",
    "## bert_keras多标签分类示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import re, os\n",
    "import codecs\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "config_path = 'model/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'model/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'model/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "\n",
    "\n",
    "neg = pd.read_excel('./data/bert_keras/neg.xls', header=None)\n",
    "pos = pd.read_excel('./data/bert_keras/pos.xls', header=None)\n",
    "\n",
    "data = []\n",
    "\n",
    "for d in neg[0]:\n",
    "    data.append((d, 0))\n",
    "\n",
    "for d in pos[0]:\n",
    "    data.append((d, 1))\n",
    "\n",
    "\n",
    "# 按照9:1的比例划分训练集和验证集\n",
    "random_order = list(range(len(data)))\n",
    "np.random.shuffle(random_order)\n",
    "train_data = [data[j] for i, j in enumerate(random_order) if i % 10 != 0]\n",
    "valid_data = [data[j] for i, j in enumerate(random_order) if i % 10 == 0]\n",
    "\n",
    "\n",
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=32):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, Y = [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0][:maxlen]\n",
    "                x1, x2 = tokenizer.encode(first=text)\n",
    "                y = d[1]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                Y.append([y])\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    Y = seq_padding(Y)\n",
    "                    yield [X1, X2], Y\n",
    "                    [X1, X2, Y] = [], [], []\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "x1_in = Input(shape=(None,))\n",
    "x2_in = Input(shape=(None,))\n",
    "\n",
    "x = bert_model([x1_in, x2_in])\n",
    "x = Lambda(lambda x: x[:, 0])(x)\n",
    "p = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([x1_in, x2_in], p)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-5), # 用足够小的学习率\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_D = data_generator(train_data)\n",
    "valid_D = data_generator(valid_data)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_D.__iter__(),\n",
    "    steps_per_epoch=len(train_D),\n",
    "    epochs=5,\n",
    "    validation_data=valid_D.__iter__(),\n",
    "    validation_steps=len(valid_D)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## BERT\n",
    "val_loss: 0.0527 - val_micro_f1: 0.9258 - val_macro_f1: 0.8505  \n",
    "test: 0.05427, 0.9299, 0.8615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import re, os\n",
    "import codecs\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "config_path = 'model/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'model/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'model/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "token_dict = {}\n",
    "\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrange word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def count_words(s):\n",
    "    stop_words = ['$', '?', '_', '“', '”', '、', '。', '《', '》','，','（', '）', '\\n', '的', '了', '是']\n",
    "    tokenstr = []\n",
    "    result = {}\n",
    "    \n",
    "    word2pos = {}\n",
    "    pos2word = {}\n",
    "    \n",
    "    words = jieba.cut(s)\n",
    "    \n",
    "    i = 0 \n",
    "    for word in words:\n",
    "        if word in stop_words: continue\n",
    "        tokenstr.append(word)\n",
    "        result[word] = result.get(word, 0) + 1\n",
    "        pos2word[i] = word\n",
    "        \n",
    "        indexs = word2pos.get(word, [])\n",
    "        indexs.append(i)\n",
    "        word2pos[word] = indexs\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    result = dict(sorted(result.items(), key=lambda x: (x[1],x[0]), reverse=True))\n",
    "    wordslist = list(result.keys())\n",
    "    assert len(set(tokenstr)) == len(wordslist)\n",
    "    return (wordslist, tokenstr, word2pos, pos2word)\n",
    "\n",
    "\n",
    "def fill_table(TD_list, related_tables,target_width, qqueue):\n",
    "    TD_list[0] = qqueue[0] # TD_list 长度为target_width 第一个位置对应此单词在wlist中的索引。0,1,2...\n",
    "    count = 1\n",
    "\n",
    "    while qqueue != [] and count < target_width:\n",
    "        use_index = qqueue[0] # 单词索引\n",
    "        del qqueue[0]\n",
    "        use_list = related_tables[use_index]  #取出use_index单词对应的相关单词。\n",
    "        len1 = len(use_list)   # 查看 i对应 的相关单词的个数。\n",
    "        len2 = target_width - count \n",
    "        if len1 >= len2:   # 大体意思应该是查看单词i对应的相关单词个数如果满足 target_width就直接从相关单词按顺序取出来填充到TD_list中。\n",
    "            TD_list[count:] = use_list[:len2]  \n",
    "            assert len(TD_list) == target_width\n",
    "            count = target_width\n",
    "            break\n",
    "        else:              # 如果不满足就有多少填多少。剩下的用 -1填充。\n",
    "            TD_list[count:count + len1] = use_list\n",
    "            assert len(TD_list) == target_width\n",
    "            count += len1\n",
    "            for next_id in use_list:\n",
    "                qqueue.append(next_id)\n",
    "    for i in range(count, target_width):\n",
    "        TD_list[i] = -1\n",
    "        \n",
    "        \n",
    "def reorder(table, word2pos, pos2word, wlist, word2id):\n",
    "    sort_table = []\n",
    "    topn, neighbor = np.array(table).shape\n",
    "    for i in range(topn):\n",
    "        tmp = []\n",
    "        tmp += word2pos[wlist[table[i][0]]] # record each center word index\n",
    "        length = len(tmp)                   # occurred times of center words\n",
    "        t = []                              # t is use to related words index\n",
    "        for j in range(1, neighbor):\n",
    "            t += word2pos[wlist[table[i][j]]]\n",
    "        index = np.random.randint(len(t), size = 20-length)\n",
    "        t = np.array(t)\n",
    "        t = list(t[index])\n",
    "        tmp = tmp + t                       # conccat the index of center word and index of its related words\n",
    "        tmp.sort()\n",
    "        for j in range(len(tmp)):\n",
    "            tmp[j] = word2id[pos2word[tmp[j]]] # convert index to word_id\n",
    "            # tmp[j] = pos2word[tmp[j]]       # convert index to word\n",
    "        sort_table.append(tmp)\n",
    "    \n",
    "    return np.array(sort_table)\n",
    "\n",
    "\n",
    "def text2matrix(s, sliding_window=3, target_width=5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    (wlist, tokenwords, word2pos, pos2word) = count_words(s)\n",
    "    word2id = {k:v for k,v in zip(wlist, range(len(wlist)))}\n",
    "    wordslist_length = len(wlist)\n",
    "    \n",
    "    AM_table = [[0 for i in range(wordslist_length)] for j in range(wordslist_length)]\n",
    "    \n",
    "    # generate occurred matrix with sliding_window\n",
    "    for num in range(len(tokenwords)-sliding_window+1):\n",
    "        for i in range(sliding_window-1):\n",
    "            for j in range(i+1, sliding_window):\n",
    "                AM_table[wlist.index(tokenwords[num + i])][wlist.index(tokenwords[num + j])] += 1\n",
    "                AM_table[wlist.index(tokenwords[num + j])][wlist.index(tokenwords[num + i])] += 1\n",
    "                \n",
    "    related_tables = {}\n",
    "    for i in range(wordslist_length):\n",
    "        related_tables[i] = [[index, num] for index, num in enumerate(AM_table[i]) if num > 0 and index != i]\n",
    "        related_tables[i].sort(key=lambda x: x[1], reverse=True)\n",
    "        related_tables[i] = [element[0] for element in related_tables[i]]\n",
    "    \n",
    "    TD_table = [[-1 for i in range(target_width)] for j in range(wordslist_length)]\n",
    "    for i in range(wordslist_length):\n",
    "        fill_table(TD_table[i], related_tables, target_width, [i]) # fill TD table with -1\n",
    "    \n",
    "    # TD_table = reorder(TD_table, word2pos, pos2word, wlist, word2id)\n",
    "    \n",
    "    # convert id to words: arrange word matrix\n",
    "    awm = []\n",
    "    for row in TD_table:\n",
    "        awm.append([pos2word[i] for i in row])\n",
    "    return wlist, awm # ,TD_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data with multi labels(Baidu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别\n",
    "n_class = 19\n",
    "# 是否处理数据不平衡\n",
    "imbalance = True\n",
    "fixed_nums = 900\n",
    "# 是否使用Arrange word matrix\n",
    "AWM = True\n",
    "\n",
    "\n",
    "x,y = None, None\n",
    "if n_class == 13:\n",
    "    x = open('./data/kkb/x.txt','r',encoding='utf8').readlines()\n",
    "    y = eval(open('./data/kkb/y.txtn_class', 'r',encoding='utf8').readlines()[0])\n",
    "elif n_class == 19:\n",
    "    x = open('./data/kkb/x_19.txt','r',encoding='utf8').readlines()\n",
    "    y = eval(open('./data/kkb/y_19.txt','r',encoding='utf8').readlines()[0])\n",
    "\n",
    "print('{} class task, data size:{},{}'.format(n_class,len(x),len(y)))\n",
    "\n",
    "\n",
    "# 类别不平衡\n",
    "if imbalance:\n",
    "    ys = [str(i) for i in y]\n",
    "    from collections import Counter\n",
    "    counts = Counter(ys)\n",
    "    for k,v in counts.items():\n",
    "        print(k,':',v)\n",
    "\n",
    "    df = pd.DataFrame({'x':x,'y':ys})\n",
    "    new_x,new_y = [],[]\n",
    "    for k, v in counts.items():\n",
    "        x_ = np.random.choice(df[df.y == k].x, fixed_nums)\n",
    "        y_ = [eval(k)]*900\n",
    "        new_x.extend(x_)\n",
    "        new_y.extend(y_)\n",
    "    print('after deal data imbalance, data size:',len(new_x),len(new_y))\n",
    "    x,y = new_x, new_y\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(range(n_class))\n",
    "y = mlb.fit_transform(y)\n",
    "y = [list(i) for i in y]\n",
    "\n",
    "# arrage word matrix\n",
    "x_awm = []\n",
    "if AWM:\n",
    "    for row in x[:10]:\n",
    "        _, awm = text2matrix(row)\n",
    "        x_awm.append(list(np.reshape(awm[:20],(-1))))\n",
    "x = x_awm\n",
    "\n",
    "\n",
    "data = list(zip(x,y))\n",
    "np.random.shuffle(data)\n",
    "train_data = data[:-4000]\n",
    "val_data = data[-4000:-2000]\n",
    "test_data = data[-2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=32, multi_labels=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.multi_labels = multi_labels\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, Y = [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0][:maxlen]\n",
    "                x1, x2 = tokenizer.encode(first=text)\n",
    "                y = d[1]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                Y.append([y])\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    Y = seq_padding(Y)\n",
    "                    if self.multi_labels: Y = Y.reshape(-1, np.shape(Y)[-1])\n",
    "                    yield [X1, X2], Y\n",
    "                    [X1, X2, Y] = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1(y_true, y_pred):\n",
    "    \"\"\"F1 metric.\n",
    "    \n",
    "    Computes the micro_f1 and macro_f1, metrics for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n",
    "    \n",
    "    \"\"\"Macro_F1 metric.\n",
    "    \"\"\"\n",
    "    precision = true_positives/(predicted_positives+K.epsilon())\n",
    "    recall = true_positives/(possible_positives+K.epsilon())\n",
    "    macro_f1 = K.mean(2*precision*recall/(precision+recall+K.epsilon()))\n",
    "        \n",
    "    \"\"\"Micro_F1 metric.\n",
    "    \"\"\"\n",
    "    precision = K.sum(true_positives)/K.sum(predicted_positives)\n",
    "    recall = K.sum(true_positives)/K.sum(possible_positives)\n",
    "    micro_f1 = 2*precision*recall/(precision+recall+K.epsilon())\n",
    "    return micro_f1\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    \"\"\"F1 metric.\n",
    "    \n",
    "    Computes the micro_f1 and macro_f1, metrics for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n",
    "    \n",
    "    \"\"\"Macro_F1 metric.\n",
    "    \"\"\"\n",
    "    precision = true_positives/(predicted_positives+K.epsilon())\n",
    "    recall = true_positives/(possible_positives+K.epsilon())\n",
    "    macro_f1 = K.mean(2*precision*recall/(precision+recall+K.epsilon()))\n",
    "        \n",
    "    \"\"\"Micro_F1 metric.\n",
    "    \"\"\"\n",
    "    precision = K.sum(true_positives)/K.sum(predicted_positives)\n",
    "    recall = K.sum(true_positives)/K.sum(possible_positives)\n",
    "    micro_f1 = 2*precision*recall/(precision+recall+K.epsilon())\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "x1_in = Input(shape=(None,))\n",
    "x2_in = Input(shape=(None,))\n",
    "\n",
    "x = bert_model([x1_in, x2_in])\n",
    "x = Lambda(lambda x: x[:, 0])(x)\n",
    "p = Dense(13, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([x1_in, x2_in], p)\n",
    "# val_metric = Metrics([val_x,val_y])\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-5), # 用足够小的学习率\n",
    "    metrics=[micro_f1,macro_f1]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = data_generator(train_data,multi_labels=True)\n",
    "valid_D = data_generator(val_data,multi_labels=True)\n",
    "test_D = data_generator(test_data,multi_labels=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_D.__iter__(),\n",
    "    steps_per_epoch=len(train_D),\n",
    "    epochs=1,\n",
    "    validation_data=valid_D.__iter__(),\n",
    "    validation_steps=len(valid_D),\n",
    "    # callbacks=[val_metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_D = data_generator(test_data,multi_labels=True)\n",
    "model.evaluate_generator(test_D.__iter__(), len(test_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 效果汇总\n",
    "\n",
    "|数据集|模型|类别|Acc|Micro-F1|Macro-F1|备注|\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|Baidu|ERNIE|2|0.73|-|-|single classify|\n",
    "|Baidu|BERT|13|-|0.9299|0.8615|multi_labels classify 13|\n",
    "|Baidu|BERT|19|-|0.8996 |0.6797|multi_labels classify 19|\n",
    "|Biadu|FastText|19|-|0.42|0.21|multi_labels classify 19(imbalance)|\n",
    "|Baidu|GCN-BERT|19|-|0.90|0.78|multi_labels classify 19(balance)|\n",
    "|Baidu|GCN-BERT|19|-|0.89|0.69|multi_labels classify 19(imbalance)|\n",
    "|Baidu|FastText|95|-|0.421|0.234|epoch 1000, ngram 5, dim 50|\n",
    "|Baidu|TextCnn|95|-|0.00478|0.028|epoch 10, lr 0.005, padding 128|\n",
    "|Baidu|BERT|21|0.7958|-|-|BERT 4 labels result|\n",
    "|Baidu|BERT|95|0.5788|0.917|0.781|only BERT|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
