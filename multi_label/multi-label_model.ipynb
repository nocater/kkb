{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size*top_n, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size*top_n, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size*top_n, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_base(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Encoder_base, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder_base(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=True, mask):\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, False)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=500, num_heads=5, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 20, 10*50)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "**Macro-f1**:  \n",
    "$\\text{Macro}-F_{1}=\\frac{1}{|\\mathcal{S}|} \\sum_{t \\in \\mathcal{S}} \\frac{2 P_{t} R_{t}}{P_{t}+R_{t}} \\\\\n",
    "P_{t}=\\frac{T P_{t}}{T P_{t}+F P_{t}} \\\\\n",
    "R_{t}=\\frac{T P_{t}}{T P_{t}+F N_{t}}$  \n",
    "\n",
    "\n",
    "**Micro-f1**:  \n",
    "$Micro-F_{1}=\\frac{2 P R}{P+R} \\\\ \n",
    "\\text{Precision(P)}=\\frac{\\sum_{t \\in \\mathcal{S}} T P_{t}}{\\sum_{t \\in \\mathcal{S}} T P_{t}+F P_{t}} \\\\\n",
    "\\text{Recall}(R)=\\frac{\\sum_{t \\in \\mathcal{S}} T P_{t}}{\\sum_{t \\in \\mathcal{S}} T P_{t}+F N_{t}}\n",
    "$\n",
    "\n",
    "``` ptyhon\n",
    "y_true = np.array([[1,0,1,1,0],[1,1,0,1,1]])\n",
    "y_pred = np.array([[0,1,1,1,0],[1,1,1,0,1]])\n",
    "\n",
    "macro_f1 0.7333333333333333\n",
    "micro_f1 0.7142857142857143\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t macro_f1:0.733333, micro_f1:0.714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7333333333333333, 0.7142857142857143)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1(y_true, y_pred):\n",
    "        assert np.shape(y_true) == np.shape(y_pred)\n",
    "        \n",
    "        def div0(a, b):\n",
    "            \"\"\" ignore / 0, div0( [-1, 0, 1], 0 ) -> [0, 0, 0] \"\"\"\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                c = np.true_divide( a, b )\n",
    "                c[ ~ np.isfinite( c )] = 0  # -inf inf NaN\n",
    "            return c\n",
    "        \n",
    "        tp = np.sum(y_true * y_pred, axis = 0)\n",
    "        tpfp = np.sum(y_true, axis=0)\n",
    "        tpfn = np.sum(y_pred, axis=0)\n",
    "\n",
    "        \"\"\"Macro_F1 metric.\n",
    "        \n",
    "        for each label, compute a binary classification f1, and then,\n",
    "        average all of them.\n",
    "        \"\"\"\n",
    "        p = div0(tp,tpfp)\n",
    "        r = div0(tp,tpfn)\n",
    "        macro_f1 = np.mean(div0(2*p*r,(p+r)))\n",
    "        \n",
    "        \"\"\"Micro_F1 metric.\n",
    "        \n",
    "        Statitic all TPs, FPs, FNs, and then use f1 fomulation.\n",
    "        \n",
    "        \"\"\"\n",
    "        p = np.sum(tp)/np.sum(tpfp)\n",
    "        r = np.sum(tp)/np.sum(tpfn)\n",
    "        micro_f1 = 2*p*r/(p+r)\n",
    "        \n",
    "        print(' \\t macro_f1:%f, micro_f1:%f' % (macro_f1, micro_f1))\n",
    "        return macro_f1, micro_f1\n",
    "    \n",
    "y_true = np.array([[1,0,1,1,0],[1,1,0,1,1]])\n",
    "y_pred = np.array([[0,1,1,1,0],[1,1,1,0,1]])\n",
    "\n",
    "f1(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "class Metrics(Callback):\n",
    "    \n",
    "    # model.fit can't support auto set valdidation_data\n",
    "    # handle set val_data in __init__\n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        \n",
    "    \n",
    "    def f1(self, y_true, y_pred):\n",
    "        assert np.shape(y_true) == np.shape(y_pred)\n",
    "        \n",
    "        def div0(a, b):\n",
    "            \"\"\" ignore / 0, div0( [-1, 0, 1], 0 ) -> [0, 0, 0] \"\"\"\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                c = np.true_divide( a, b )\n",
    "                c[ ~ np.isfinite( c )] = 0  # -inf inf NaN\n",
    "            return c\n",
    "        \n",
    "        tp = np.sum(y_true * y_pred, axis = 0)\n",
    "        tpfp = np.sum(y_true, axis=0)\n",
    "        tpfn = np.sum(y_pred, axis=0)\n",
    "\n",
    "        \"\"\"Macro_F1 metric.\n",
    "        \n",
    "        for each label, compute a binary classification f1, and then,\n",
    "        average all of them.\n",
    "        \"\"\"\n",
    "        p = div0(tp,tpfp)\n",
    "        r = div0(tp,tpfn)\n",
    "        macro_f1 = np.mean(div0(2*p*r,(p+r)))\n",
    "        \n",
    "        \"\"\"Micro_F1 metric.\n",
    "        \n",
    "        Statitic all TPs, FPs, FNs, and then use f1 fomulation.\n",
    "        \n",
    "        \"\"\"\n",
    "        p = np.sum(tp)/np.sum(tpfp)\n",
    "        r = np.sum(tp)/np.sum(tpfn)\n",
    "        micro_f1 = 2*p*r/(p+r)\n",
    "        \n",
    "        print(' \\t macro_f1:%f, micro_f1:%f' % (macro_f1, micro_f1))\n",
    "        return macro_f1, micro_f1\n",
    "\n",
    "    \n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.val_micro_f1s = []\n",
    "        self.val_macro_f1s = []\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(\n",
    "            self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        _val_macro_f1,_val_micro_f1 = self.f1(val_targ, val_predict)\n",
    "        self.val_macro_f1s.append(_val_macro_f1)\n",
    "        self.val_micro_f1s.append(_val_micro_f1)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Margin loss\n",
    "**orgin loss**:  \n",
    "$\\begin{aligned} \\mathcal{L}=\\sum_{k=1}^{L}[ & T_{k} \\max \\left(0, m^{+}-\\left\\|v_{k}\\right\\|\\right)^{2} \\\\ &\\left.+\\lambda \\cdot p \\cdot \\alpha_{k} \\cdot\\left(1-T_{k}\\right) \\cdot \\max \\left(0,\\left\\|v_{k}\\right\\|-m^{-}\\right)^{2}\\right] \\end{aligned}$\n",
    "\n",
    "**used loss**(it seems doesn't work):  \n",
    "$\\begin{aligned} \\mathcal{L}=\\sum_{k=1}^{L}[ & T_{k} \\max \\left(0, m^{+}-\\left\\|v_{k}\\right\\|\\right)^{2} \\\\ &\\left.+\\lambda  \\cdot\\left(1-T_{k}\\right) \\cdot \\max \\left(0,\\left\\|v_{k}\\right\\|-m^{-}\\right)^{2}\\right] \\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def margin_loss(y_true, y_pred, m_plus=0.9, m_minus=0.1, lambd = 0.5):\n",
    "    \n",
    "    l = y_true * np.clip(m_plus-y_pred,0,1)\n",
    "    l = np.sum(l**2, axis=1, keepdims=True)\n",
    "    \n",
    "    # alpha is not finished\n",
    "    alpha = None\n",
    "    \n",
    "    r = y_true * np.clip(y_pred-m_minus,0,1)\n",
    "    r = lambd * np.sum(r**2, axis=1, keepdims=True)\n",
    "    \n",
    "    loss = np.mean(l+r)\n",
    "    return loss\n",
    "\"\"\"\n",
    "\n",
    "def margin_loss(y_true, y_pred, m_plus=0.9, m_minus=0.1, lambd = 0.5):\n",
    "    \n",
    "    l = y_true * tf.keras.backend.clip(m_plus-y_pred,0,1)\n",
    "    l = tf.keras.backend.sum(l**2, axis=1, keepdims=True)\n",
    "    \n",
    "    # alpha is not finished\n",
    "    alpha = None\n",
    "\n",
    "    r = y_true * tf.keras.backend.clip(y_pred-m_minus,0,1)\n",
    "    r = lambd * tf.keras.backend.sum(r**2, axis=1, keepdims=True)\n",
    "    \n",
    "    loss = tf.keras.backend.mean(l+r)\n",
    "    return loss\n",
    "\n",
    "# margin_loss(np.array([[0,1,1],[1,0,0]]),np.array([[0.2,0.5,0.6],[0.8,0.1,0.2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = seq_lens*embedding\n",
    "topn = 100\n",
    "seq_lens = 20\n",
    "embedding = 50\n",
    "\n",
    "num_layers = 1\n",
    "d_model = seq_lens * embedding\n",
    "dff = 512\n",
    "num_heads = 10\n",
    "num_labels = 126\n",
    "dff = 1024\n",
    "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# way 1:Model Sequential\n",
    "\n",
    "use **Encoder_ours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-- None\n",
      " \t macro_f1:0.297997, micro_f1:0.466227\n",
      "32/32 [==============================] - 63s 2s/sample - loss: 0.9173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.29799726758299655], [0.46622734761120266])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "tff = tf.keras.Sequential()\n",
    "tff.add(tf.keras.layers.Reshape((topn,d_model)))\n",
    "tff.add(Encoder(num_layers, d_model, num_heads, dff, dropout_rate))\n",
    "tff.add(tf.keras.layers.Flatten())\n",
    "tff.add(tf.keras.layers.Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "tff.compile(loss = 'binary_crossentropy', # margin_loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "           )\n",
    "\n",
    "x = np.random.uniform(size=(42,topn,seq_lens,embedding)) # batch,topn,sequence_length,embdding_dim\n",
    "y = np.random.randint(2,size=(42,num_labels))\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y = mlb.fit_transform(y)\n",
    "\n",
    "# dont use auto split train valid dataset in .fit\n",
    "val_metric = Metrics([x[-10:],y[-10:]])\n",
    "his = tff.fit(x[:-10],y[:-10],\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          callbacks=[val_metric])\n",
    "\n",
    "# tff.summary()\n",
    "val_metric.val_macro_f1s, val_metric.val_micro_f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# way 2:Model functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t macro_f1:0.501162, micro_f1:0.599625\n",
      "32/32 [==============================] - 67s 2s/sample - loss: 22.1047\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Multi-label Model\n",
    "\"\"\"\n",
    "class MLM(tf.keras.Model):\n",
    "    def __init__(self, topn, num_labels, num_layers, d_model, num_heads, dff, \n",
    "               rate=0.1):\n",
    "        super(MLM, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.reshape = tf.keras.layers.Reshape((topn, d_model))\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        self.out = tf.keras.layers.Dense(num_labels, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training, mask=False):\n",
    "        x = self.reshape(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        fla = tf.keras.layers.Flatten()\n",
    "        x = fla(x)\n",
    "        x = self.out(x)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "mlm = MLM(topn, num_labels, num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "mlm.compile(loss = margin_loss, #'binary_crossentropy', # margin_loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
    "\n",
    "x = np.random.uniform(size=(42,topn,seq_lens,embedding)) # batch,topn,sequence_length,embdding_dim\n",
    "y = np.random.randint(2,size=(42,num_labels))\n",
    "\n",
    "\n",
    "# dont use auto split train valid dataset in .fit\n",
    "\n",
    "val_metric = Metrics([x[-10:],y[-10:]])\n",
    "his = mlm.fit(x[:-10],y[:-10],\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          callbacks=[val_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = r'/mnt/d/Dataset/rcv1/'\n",
    "\n",
    "# for file in os.listdir(PATH+'matrix/'):\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "for i in range(2000, 28000, 2000):\n",
    "    _x = np.load('./data/rcv1/matrix/range'+str(i)+'_'+str(i+2000)+'.npy')\n",
    "    _y = np.load('./data/rcv1/label/label_range'+str(i)+'_'+str(i+2000)+'.npy')\n",
    "    if len(x)>0:\n",
    "        x = np.append(x,_x,axis=0)\n",
    "        y = np.append(y,_y,axis=0)\n",
    "    else:\n",
    "        x = _x\n",
    "        y = _y\n",
    "    \n",
    "indexs = list(range(len(x)))\n",
    "np.random.shuffle(indexs)\n",
    "x = x[indexs]\n",
    "y = y[indexs]\n",
    "print(x.shape,y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
