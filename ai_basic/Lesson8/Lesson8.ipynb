{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：https://github.com/fatecbf/toutiao-text-classfication-dataset/blob/master/toutiao_cat_data.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎的三个步骤：\n",
    "- 自动下载网页(网页爬虫)\n",
    "- 建立索引\n",
    "- 度量网页质量(pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./toutiao_cat_data.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6552368441838272771_!_101_!_news_culture_!_发酵床的垫料种类有哪些？哪种更好？_!_\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d.split('_!_') for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['id', 'code', 'category', 'content', 'keys'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跑不动就减小数据集\n",
    "# df = df[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6551700932705387022</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>京城 最 值得 你 来场 文化 之旅 的 博物馆</td>\n",
       "      <td>保利集团,马未都,中国科学技术馆,博物馆,新中国\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552368441838272771</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552407965343678723</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552332417753940238</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552475601595269390</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>黄杨木 是 什么 树 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id code      category  \\\n",
       "0  6551700932705387022  101  news_culture   \n",
       "1  6552368441838272771  101  news_culture   \n",
       "2  6552407965343678723  101  news_culture   \n",
       "3  6552332417753940238  101  news_culture   \n",
       "4  6552475601595269390  101  news_culture   \n",
       "\n",
       "                                content                        keys  \n",
       "0              京城 最 值得 你 来场 文化 之旅 的 博物馆  保利集团,马未都,中国科学技术馆,博物馆,新中国\\n  \n",
       "1          发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？                          \\n  \n",
       "2       上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？                          \\n  \n",
       "3  林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？                          \\n  \n",
       "4                          黄杨木 是 什么 树 ？                          \\n  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_DOCUMENTS = len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.831 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6551700932705387022</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>京城 最 值得 你 来场 文化 之旅 的 博物馆</td>\n",
       "      <td>保利集团,马未都,中国科学技术馆,博物馆,新中国\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6552368441838272771</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6552407965343678723</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6552332417753940238</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6552475601595269390</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>黄杨木 是 什么 树 ？</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id code      category  \\\n",
       "0  6551700932705387022  101  news_culture   \n",
       "1  6552368441838272771  101  news_culture   \n",
       "2  6552407965343678723  101  news_culture   \n",
       "3  6552332417753940238  101  news_culture   \n",
       "4  6552475601595269390  101  news_culture   \n",
       "\n",
       "                                content                        keys  \n",
       "0              京城 最 值得 你 来场 文化 之旅 的 博物馆  保利集团,马未都,中国科学技术馆,博物馆,新中国\\n  \n",
       "1          发酵 床 的 垫料 种类 有 哪些 ？ 哪 种 更好 ？                          \\n  \n",
       "2       上联 ： 黄山 黄河 黄皮肤 黄土高原 。 怎么 对 下联 ？                          \\n  \n",
       "3  林徽因 什么 理由 拒绝 了 徐志摩 而 选择 梁思成 为 终身伴侣 ？                          \\n  \n",
       "4                          黄杨木 是 什么 树 ？                          \\n  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'] = df.content.apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建字典\n",
    "vocab = []\n",
    "for document in df.content:\n",
    "    vocab.extend(document.split())\n",
    "\n",
    "# 总单词数目\n",
    "NUM_TREMS = len(vocab)\n",
    "\n",
    "vocab = dict(Counter(vocab))\n",
    "# 根据词频排序\n",
    "vocab = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "# 转换成字典\n",
    "vocab = dict(vocab)\n",
    "# 过滤少于5000词频的单词\n",
    "vocab = {k:v for k,v in vocab.items() if v>100}\n",
    "# word2id id2word\n",
    "word2id = {k:v for v,k in enumerate(vocab)}\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "# 字典数量\n",
    "NUM_WORDS = len(vocab)\n",
    "NUM_WORDS, # vocab.keys(), word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的索引就是对每一个文档(网页)创建一个很长的二进制数字，每个数字表示其对应的关键字是否在文档当中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "way1\n",
    "\n",
    "         文档1  文档2 ...\n",
    "关键字1    1      0\n",
    "关键字2    0      1\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(x):\n",
    "    \"\"\"\n",
    "    将字符串转换成id list\n",
    "    \"\"\"\n",
    "    index = [0] * NUM_WORDS\n",
    "    words = x.split()\n",
    "    for word in words:\n",
    "        i = word2id.get(word, -1)\n",
    "        if i == -1: continue\n",
    "        index[i] = 1\n",
    "    return index\n",
    "\n",
    "\n",
    "# 验证一下是否是想要的结果\n",
    "r = generate_index('京城 最 值得 你 来场 文化 之旅 的 博物馆')\n",
    "# [id2word[int(w)] for w in \"\"\"35\n",
    "# 203\n",
    "# 7\n",
    "# 394\n",
    "# 2002\n",
    "# 2\n",
    "# 2180\"\"\".split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ps: 需要运行很长时间，占用很大内存条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = df.content.apply(generate_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "way2\n",
    "       关键字1  关键字2 关键字3 ...\n",
    "文档1    1        0       0\n",
    "文档2    0        1       1\n",
    "...\n",
    "```\n",
    "将其转置一下就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 50000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = np.array(all_index.tolist()).T\n",
    "all_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样会生成一个表，即索引表：表的每一行对应一个关键字，而每一个关键字后面跟着一组数字，表示哪些文档包含这个关键字。  \n",
    "这时候，当我们查询某个关键字的时候，不再需要将整个文档集合从头到尾查看一遍，只需要查找这个表就知道应该取哪些文档中去搜索就可以了。  \n",
    "这和我们生活中的书籍的目录是非常相似的。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们想要查询多个关键字的文档时候，也只需要将两个关键字对应的索引向量进行一个布尔运算(and)，便能将所有出现两个关键字的文档检索出来。  \n",
    "计算机对于布尔运算的计算速度是非常快的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如互联网有$100亿^3$($10^{10}$)个有用的网页，再假设词汇表有30万个单词(肯定偏小)，那么索引的大小至少是100亿x30万=3000万亿。  \n",
    "假如我们去掉一些只出现一个网页(文档)中的词，取百分之一，那也有30万亿。  \n",
    "\n",
    "不仅这样，索引除了保存关键字信息，还要有一些其他信息需要保存，比如词出现的位置啦，次数啦，  \n",
    "这些数据一台服务器是村放不下的，所以通常把索引切片，放到不同服务器上，这就和服务器的集群，分布式很相关了。\n",
    "\n",
    "将数据存到不同的服务器上，就会引申出很多其它问题，比如，如何保证数据的一致性(数据脏读等等)，如何降低服务期间的通信成本，否则分布式效率还不如一台服务器效果高。\n",
    "\n",
    "总之，由搜索引擎的索引可以扩展到很多很深的知识，这些知识偏向工程化，但工程化的问题不管多么复杂，至少在索引的原理上，还是非常简单的。类似目录，等价于布尔运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 度量网页质量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google的pagerank方法是常用的网页质量度量方法，在NLP领域中也是比较重要的一个方法。由于此数据集不适用于pagerank方法，这里就不再展开。  \n",
    "仅做简单介绍，有兴趣的同学可自行深入理解研究，或在开课吧NLP课程中进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在掌握\n",
    "- 数据下载\n",
    "- 建立索引\n",
    "- 度量网页\n",
    "\n",
    "后，我们再来看下搜索引擎的其它考量：\n",
    "1. 索引要完善，巧妇难为无米之炊，算法再好，要找的答案不再候选答案集合中，那都是徒劳的。\n",
    "2. 网页质量的度量，一些八卦性质的网站pagerank值比较高，但权威性质就比较低\n",
    "3. 用户偏好，不同的用户对结果的评判标准不一样，个性化推荐(深入就是走向推荐的方向了)\n",
    "4. 确定查询和网页的相关性\n",
    "\n",
    "OK，我们来看怎么确定网页和查询的相关性的度量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定网页和查询的相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们常用词频(Term Frequency)的概念来表示：  \n",
    "$$tf = \\frac{单词的出现频率}{网页的总字数}$$\n",
    "\n",
    "所以，一个文档中的单词出现的词频可以表示为：$TF_1, TF_2, ...$  \n",
    "那么假如给定关键字$w_1, w_2$，可以计算每个文档中两个关键字的词频，记为$TF_1, TF_2$  \n",
    "\n",
    "比如 `北京` `首都`   在文档 `可爱 的 北京 是 祖国 的 首都， 北京 是 个 美丽 的 城市` 和 文档`他 喊道 北京 北京 我爱你， 北京 北京 我 来了， 首都 北京 我 来 了`\n",
    "词频分别是\n",
    "[2/13, 1/13]\n",
    "[5/14, 1/14]\n",
    "\n",
    "所以，我们可以把查询关键字和文档的相关性的度量表示为：\n",
    "$TF_1+TF_2$\n",
    "\n",
    "当有N个关键字，M个文档时候，第i个文档的相似度可以记为：\n",
    "\n",
    "$$sim(q, d_i) = TF_1+TF_2+....TF_N, (q=w_1,w_2,...w_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['北京', '的', '房价', '多少', '？']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 北京的房价多少\n",
    "query = '北京的房价多少？'\n",
    "#query = '特朗普是什么的？'\n",
    "query = list(jieba.cut(query))\n",
    "\n",
    "# 过滤不再字典的\n",
    "query = [word for word in query if word in vocab]\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每个关键字的TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find document id that contains all keys: []\n"
     ]
    }
   ],
   "source": [
    "def querybykeys(query):\n",
    "    finds = np.array([1] * NUM_DOCUMENTS)\n",
    "    for key in query:\n",
    "        finds += all_index[word2id[key]]\n",
    "        finds = finds==2\n",
    "        finds = finds.astype(int)\n",
    "    return finds\n",
    "\n",
    "finds = querybykeys(query)\n",
    "finds = [print(i) for i,j in enumerate(finds) if j]\n",
    "print('Find document id that contains all keys:', finds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find document id that contains all keys: [11848, 21857, 23181, 23516, 25454, 28720, 29154, 45713]\n",
      "房价 白跌 了 ！ 北京 首 套房 贷款 利率 上浮 10%\n",
      "你 认为 北京 未来 5 年 的 房价 会 下跌 吗 ？\n",
      "「 重磅 」 北京 限 房价 项目 销售 办法 出炉   部分 将 收作 共有 产权 房\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n",
      "北京 房贷利率 下周 再涨   专家 ： 累计 相当于 房价 上调 了 10%\n",
      "北京 限价 房转 共有 产权 房 政策 拟 出炉   对 房价 有何 影响 ？\n",
      "房价 不涨 卖 不动 ， 北京 首 套房 利率 继续 上浮 ， 刚需 招惹 了 谁 ？\n",
      "北京 的 房价 还会 继续 上涨 吗 ？\n"
     ]
    }
   ],
   "source": [
    "# 既然没有那咱找几个小的数据进行继续的说明\n",
    "finds = querybykeys(['北京', '房价']) # ['北京', '房价']\n",
    "finds = [i for i,j in enumerate(finds) if j]\n",
    "print('Find document id that contains all keys:', finds)\n",
    "\n",
    "# 将这些数据打印出来\n",
    "for d in finds:\n",
    "    print(df.iloc[d].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房价', '白跌', '了', '！', '北京', '首', '套房', '贷款', '利率', '上浮', '10%']\n",
      "{'房价': 0.09090909090909091, '北京': 0.09090909090909091}\n",
      "0.18181818181818182\n",
      "['你', '认为', '北京', '未来', '5', '年', '的', '房价', '会', '下跌', '吗', '？']\n",
      "{'北京': 0.08333333333333333, '的': 0.08333333333333333, '房价': 0.08333333333333333}\n",
      "0.25\n",
      "['「', '重磅', '」', '北京', '限', '房价', '项目', '销售', '办法', '出炉', '部分', '将', '收作', '共有', '产权', '房']\n",
      "{'北京': 0.0625, '房价': 0.0625}\n",
      "0.125\n",
      "['北京', '的', '房价', '还会', '继续', '上涨', '吗', '？']\n",
      "{'北京': 0.125, '的': 0.125, '房价': 0.125}\n",
      "0.375\n",
      "['北京', '房贷利率', '下周', '再涨', '专家', '：', '累计', '相当于', '房价', '上调', '了', '10%']\n",
      "{'北京': 0.08333333333333333, '房价': 0.08333333333333333}\n",
      "0.16666666666666666\n",
      "['北京', '限价', '房转', '共有', '产权', '房', '政策', '拟', '出炉', '对', '房价', '有何', '影响', '？']\n",
      "{'北京': 0.07142857142857142, '房价': 0.07142857142857142}\n",
      "0.14285714285714285\n",
      "['房价', '不涨', '卖', '不动', '，', '北京', '首', '套房', '利率', '继续', '上浮', '，', '刚需', '招惹', '了', '谁', '？']\n",
      "{'房价': 0.0625, '北京': 0.0625}\n",
      "0.125\n",
      "['北京', '的', '房价', '还会', '继续', '上涨', '吗', '？']\n",
      "{'北京': 0.125, '的': 0.125, '房价': 0.125}\n",
      "0.375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.09090909090909091, 0.09090909090909091],\n",
       " [0.08333333333333333, 0.08333333333333333, 0.08333333333333333],\n",
       " [0.0625, 0.0625],\n",
       " [0.125, 0.125, 0.125],\n",
       " [0.08333333333333333, 0.08333333333333333],\n",
       " [0.07142857142857142, 0.07142857142857142],\n",
       " [0.0625, 0.0625],\n",
       " [0.125, 0.125, 0.125]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算每个数据的TF\n",
    "tf_result = []\n",
    "for d in finds:\n",
    "    words = df.iloc[d].content.split()\n",
    "    # 统计单词出现次数\n",
    "    tf = dict(Counter(words))\n",
    "    # 统计词频\n",
    "    tf = {k:v/len(tf) for k,v in tf.items() if k in query}\n",
    "    # 统计query的词频\n",
    "    print(words)\n",
    "    print(tf)\n",
    "    print(sum(tf.values()))\n",
    "    tf_result.append(list(tf.values()))\n",
    "    \n",
    "tf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里面大家有没有发现一个问题？  \n",
    "因为数据量比较少，可能不太明显，  \n",
    "但你还是能够感受得到的。\n",
    "\n",
    "在sim最大的两条数据中，`北京` `房价` `的` 三个单词分别贡献了0.125的权重  \n",
    "但从直观理解来说，`的`字对于问题其实完全没有帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = dict(Counter(vocab))\n",
    "tf = {k:round(v/words,8) for k,v in tf.items()}\n",
    "tf = sorted(tf.items(), key=lambda x:x[1], reverse=True)\n",
    "# tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
